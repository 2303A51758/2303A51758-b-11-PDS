{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsuNZaFv8QuIt300SwAlOF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51758/2303A51758-b-11-PDS/blob/main/reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw4WIx-5Cevz",
        "outputId": "142582d1-dbab-4e22-e362-e06f9cdba585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All RL algorithms implemented and trained!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "from gym import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Custom RL Environment\n",
        "# -------------------------------\n",
        "class SocialMediaEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(SocialMediaEnv, self).__init__()\n",
        "\n",
        "        # Example state: [hours_on_social_media, GPA, sleep_hours, stress_level]\n",
        "        self.observation_space = spaces.Box(low=np.array([0, 0, 0, 0]),\n",
        "                                            high=np.array([12, 10, 12, 10]),\n",
        "                                            dtype=np.float32)\n",
        "\n",
        "        # Actions: 0 = No change, 1 = Reduce usage, 2 = Awareness program\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        self.state = None\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        hours, gpa, sleep, stress = self.state\n",
        "\n",
        "        # Simulate action effects\n",
        "        if action == 1:  # reduce usage\n",
        "            hours = max(0, hours - 1)\n",
        "            gpa = min(10, gpa + 0.1)\n",
        "            stress = max(0, stress - 0.1)\n",
        "        elif action == 2:  # awareness program\n",
        "            hours = max(0, hours - 0.5)\n",
        "            sleep = min(12, sleep + 0.2)\n",
        "\n",
        "        # Reward: better GPA & sleep, lower stress\n",
        "        reward = gpa + sleep - hours - stress\n",
        "        self.state = np.array([hours, gpa, sleep, stress])\n",
        "        done = gpa >= 9.5 or sleep >= 10  # stop if very good condition\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([random.randint(3, 8),  # hours\n",
        "                               random.uniform(4, 7), # GPA\n",
        "                               random.uniform(5, 8), # sleep\n",
        "                               random.uniform(3, 6)]) # stress\n",
        "        return self.state\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Q-Learning\n",
        "# -------------------------------\n",
        "def q_learning(env, episodes=500):\n",
        "    q_table = np.zeros((20, 20, 20, 20, env.action_space.n))  # discretized\n",
        "\n",
        "    def discretize(state):\n",
        "        return tuple((state // 1).astype(int))  # bucket\n",
        "\n",
        "    alpha, gamma, epsilon = 0.1, 0.9, 0.2\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        d_state = discretize(state)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_table[d_state])\n",
        "\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            d_new = discretize(new_state)\n",
        "\n",
        "            q_table[d_state][action] = (1 - alpha) * q_table[d_state][action] + alpha * (reward + gamma * np.max(q_table[d_new]))\n",
        "\n",
        "            d_state = d_new\n",
        "    return q_table\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: SARSA\n",
        "# -------------------------------\n",
        "def sarsa(env, episodes=500):\n",
        "    q_table = np.zeros((20, 20, 20, 20, env.action_space.n))\n",
        "\n",
        "    def discretize(state):\n",
        "        return tuple((state // 1).astype(int))\n",
        "\n",
        "    alpha, gamma, epsilon = 0.1, 0.9, 0.2\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        d_state = discretize(state)\n",
        "\n",
        "        action = env.action_space.sample()\n",
        "        done = False\n",
        "        while not done:\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            d_new = discretize(new_state)\n",
        "\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                new_action = env.action_space.sample()\n",
        "            else:\n",
        "                new_action = np.argmax(q_table[d_new])\n",
        "\n",
        "            q_table[d_state][action] += alpha * (reward + gamma * q_table[d_new][new_action] - q_table[d_state][action])\n",
        "\n",
        "            d_state, action = d_new, new_action\n",
        "    return q_table\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Deep Q-Network (DQN)\n",
        "# -------------------------------\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def train_dqn(env, episodes=500):\n",
        "    model = DQN(env.observation_space.shape[0], env.action_space.n)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    gamma, epsilon = 0.9, 0.2\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = torch.tensor(env.reset(), dtype=torch.float32)\n",
        "        done = False\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = torch.argmax(model(state)).item()\n",
        "\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            new_state = torch.tensor(new_state, dtype=torch.float32)\n",
        "\n",
        "            target = reward + gamma * torch.max(model(new_state)).item()\n",
        "            pred = model(state)[action]\n",
        "\n",
        "            loss = criterion(pred, torch.tensor(target, dtype=torch.float32))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Policy Gradient (REINFORCE)\n",
        "# -------------------------------\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def reinforce(env, episodes=500):\n",
        "    policy = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=0.001)\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = torch.tensor(env.reset(), dtype=torch.float32)\n",
        "        rewards, log_probs = [], []\n",
        "        done = False\n",
        "        while not done:\n",
        "            probs = policy(state)\n",
        "            action = torch.multinomial(probs, 1).item()\n",
        "            log_prob = torch.log(probs[action])\n",
        "\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            log_probs.append(log_prob)\n",
        "            state = torch.tensor(new_state, dtype=torch.float32)\n",
        "\n",
        "        returns = torch.tensor(sum(rewards), dtype=torch.float32)\n",
        "        loss = -sum([log_prob * returns for log_prob in log_probs])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# -------------------------------\n",
        "# Step 6: Actor-Critic\n",
        "# -------------------------------\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.fc = nn.Linear(state_size, 64)\n",
        "        self.actor = nn.Linear(64, action_size)\n",
        "        self.critic = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc(x))\n",
        "        return torch.softmax(self.actor(x), dim=-1), self.critic(x)\n",
        "\n",
        "def actor_critic(env, episodes=500):\n",
        "    ac = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
        "    optimizer = optim.Adam(ac.parameters(), lr=0.001)\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = torch.tensor(env.reset(), dtype=torch.float32)\n",
        "        done = False\n",
        "        while not done:\n",
        "            probs, value = ac(state)\n",
        "            action = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            new_state = torch.tensor(new_state, dtype=torch.float32)\n",
        "            _, new_value = ac(new_state)\n",
        "\n",
        "            advantage = torch.tensor(reward + (0.9 * new_value.item() if not done else 0) - value.item(), dtype=torch.float32)\n",
        "\n",
        "            actor_loss = -torch.log(probs[action]) * advantage\n",
        "            critic_loss = advantage ** 2\n",
        "            loss = actor_loss + critic_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "# -------------------------------\n",
        "# Example Run\n",
        "# -------------------------------\n",
        "env = SocialMediaEnv()\n",
        "q_learning(env)\n",
        "sarsa(env)\n",
        "train_dqn(env)\n",
        "reinforce(env)\n",
        "actor_critic(env)\n",
        "\n",
        "print(\"All RL algorithms implemented and trained!\")"
      ]
    }
  ]
}